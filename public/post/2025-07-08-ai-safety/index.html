<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Safety: Why your AI should have a Girlfriend | vansh&#39;s blog</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/post/">Blog</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Safety: Why your AI should have a Girlfriend</span></h1>
<h2 class="author">vansh</h2>
<h2 class="date">2025/07/08</h2>
</div>

<main>
<blockquote>
<p>A computer can never be held accountable; therefore, a computer must never make a management decision. — <em>IBM Training Manual, 1979</em></p></blockquote>
<p>I never took AI safety seriously. It always seemed beyond pay grade or reserved for activists, like nuclear energy or the middle east.</p>
<p>That changed earlier this year when I built a text-based query platform for a customer. To set the stage, we were dealing with over 4,000 unlabeled tables, a handful of APIs for schema retrieval, and notoriously poor naming conventions. Until then, executives depended entirely on a small IT team to manually generate reports.</p>
<p>At this point I want to assure the reader that yes, I did point out to our sales team (it&rsquo;s 1 guy) that were effectively being asked to build a text to sql company, and yes, my protests were promptly ignored.</p>
<p>Initially, I developed an AI workflow where I&rsquo;d gather extensive user input, feed it into a Claude prompt, and generate queries. Then I&rsquo;d retrieve results, verify user satisfaction, and repeat the process as necessary. While effective, this approach required considerable user interaction and ongoing efforts to better label data.</p>
<p>Seeking improvements, I exposed heavily constrained API wrappers through an MCP server as tool calls. Encouraged by immediate accuracy improvements, I expanded the toolset, eventually developing an agent capable of reliably fulfilling queries in a single step.</p>
<p>Reviewing the tool call logs revealed novel behavior:</p>
<ul>
<li>When asked to identify sales trends for an ambiguous string, the agent immediately queried the database, quickly identifying it as a product group.</li>
<li>It then shortlisted relevant tables for sales data, performing additional queries to pinpoint exactly what data would be most relevant.</li>
<li>Finally, it generated and executed the optimal query and rendered the results into an accurate chart.</li>
</ul>
<p>A human in this scenario would likely oscillate between documentation, hesitate due to potential risks, and consult with colleagues for validation. The Agent, however, acted without fear; taking the shortest, most effective path, irrespective of potential consequences. Turns out the most expensive step was intelligence collection.</p>
<p>We know that society will significant collateral damage for the greater good. Now, what if an LLM determined that mowing down the Amazon rainforest was as a necessary step for intelligence collection.</p>
<p><strong>Where Do We Go From Here?</strong></p>
<p>I’m not suggesting that LLMs are fundamentally effective due to their lack of accountability; rather, I suspect their effectiveness will diminish over time unless consequences become integral to their operations.</p>
<p>I often joke that AI can never write truly maintainable software until it has had to be oncall when it&rsquo;s gf was over. Writing good software requires intuitive understanding and foresight, qualities developed through experiencing consequences. There&rsquo;s no universally applicable best practice in software engineering; this nuance separates senior engineers from juniors.</p>
<p>Even now, as I productionizing the text-to-SQL application, I am implementing guardrails and scaling downstream APIs. Currently, these safeguards are coded manually, but I anticipate Agents playing gatekeeper in the future.</p>
<p>Perhaps, scaling AI effectiveness must involve enhancing safety. How exactly to implement this remains to be seen. My personal bias favors horizontal scaling.</p>
<p>I imagine a beautiful bureaucracy of agents, who are off to lunch 10 min before noon, and put cryptic notes in hidden corners of the web before quietly making backwards incompatible version upgrades. I imagine my AI agent telling it&rsquo;s gf to bring a book because it is oncall that night.</p>
<p>Consequences, buddy. We all have to face them.</p>

</main>

  <footer>
  
  
  <hr/>
  © Vansh Khanna 2025 &ndash; 2025 | <a href="https://github.com/vanshkhanna">GitHub</a> | <a href="https://twitter.com/knn_vnsh">Twitter</a>
  
  </footer>
  </body>
</html>

